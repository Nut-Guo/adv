{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "adv-lightning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "93196f85e7ff40a98e94696ffb273e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_06fa7b72b6e645609e0e841afc68860c",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_adbaeca5a59b4b90975589a70764ad58",
       "IPY_MODEL_08b409691db144ebaeab1a7c567fcab3"
      ]
     }
    },
    "06fa7b72b6e645609e0e841afc68860c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": "row wrap",
      "width": "100%",
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": "inline-flex",
      "left": null
     }
    },
    "adbaeca5a59b4b90975589a70764ad58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_732c0f4d99ac42ee909946318101e1f8",
      "_dom_classes": [],
      "description": "Validation sanity check:   0%",
      "_model_name": "FloatProgressModel",
      "bar_style": "danger",
      "max": 2,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 0,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_89ad58ec717a48fcb94dd1f38e7faeb9"
     }
    },
    "08b409691db144ebaeab1a7c567fcab3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_a80f1cefee004d7fa9d67622b8af49ef",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 0/2 [00:00&lt;?, ?it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_e8d681f3a6cc426a9906fc37cb425fd3"
     }
    },
    "732c0f4d99ac42ee909946318101e1f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "89ad58ec717a48fcb94dd1f38e7faeb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": "2",
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "a80f1cefee004d7fa9d67622b8af49ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "e8d681f3a6cc426a9906fc37cb425fd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "0fad82b7365249f4aeb85afedb60e44c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_54f2c4c6e833427083d96bdb1ef8ba03",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_bad5359b1b194503ad9aa39a38d36d69",
       "IPY_MODEL_b9e7e380d86342f789531674320b2137"
      ]
     }
    },
    "54f2c4c6e833427083d96bdb1ef8ba03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": "row wrap",
      "width": "100%",
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": "inline-flex",
      "left": null
     }
    },
    "bad5359b1b194503ad9aa39a38d36d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_eb939163d6c3440e9ff6ba3146ac2ef0",
      "_dom_classes": [],
      "description": "Epoch 0:   2%",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 5326,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 100,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_2eb37042f4e84086a153e0941f68c19b"
     }
    },
    "b9e7e380d86342f789531674320b2137": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_e1d8bb242d12453c89089743643770fb",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "â€‹",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 100/5326 [00:48&lt;42:32,  2.05it/s, loss=nan, v_num=21]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_880cc5ca1b72453bb97206e30c04ca28"
     }
    },
    "eb939163d6c3440e9ff6ba3146ac2ef0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "2eb37042f4e84086a153e0941f68c19b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": "2",
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "e1d8bb242d12453c89089743643770fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "880cc5ca1b72453bb97206e30c04ca28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "hd5uUKSTDn-z"
   },
   "source": [
    "# os.makedirs(CONFIG_PATH, exist_ok=True)\n",
    "# for config in CONFIGS:\n",
    "#   file_path = os.path.join(CONFIG_PATH, config)\n",
    "#   if not os.path.isfile(file_path):\n",
    "#     url_path = CONFIG_BASE_URL+config\n",
    "#     try:\n",
    "#       urllib.request.urlretrieve(url_path, file_path)\n",
    "#     except HTTPError as e:\n",
    "#       print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "# os.makedirs(WEIGHTS_PATH, exist_ok=True)\n",
    "# for weight_url in WEIGHTS:\n",
    "#   file_path = os.path.join(WEIGHTS_PATH, weight_url)\n",
    "#   if not os.path.isfile(file_path):\n",
    "#     url_path = WEIGHTS_BASE_URL + weight_url\n",
    "#     try:\n",
    "#       urllib.request.urlretrieve(url_path, file_path)\n",
    "#     except HTTPError as e:\n",
    "#       print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ga5aYmzIB-rA"
   },
   "source": [
    "# CONFIG = 'yolov4-tiny.cfg'\n",
    "# WEIGHTS = 'yolov4-tiny.weights'\n",
    "# config_path = os.path.join(CONFIG_PATH, CONFIG)\n",
    "# weights_path = os.path.join(WEIGHTS_PATH, WEIGHTS)\n",
    "# # config_path = '/content/drive/MyDrive/yolo/yolov4.cfg'\n",
    "# # weights_path = '/content/drive/MyDrive/yolo/yolov4.weights'\n",
    "# config_url = 'https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg'\n",
    "# weights_url = 'https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights'\n",
    "# download_data(config_url, \"config\")\n",
    "# download_data(weights_url, \"weights\")\n",
    "# config = YOLOConfiguration(config_path)\n",
    "# model = YOLO(config.get_network())\n",
    "# with open(weights_path, 'rb') as f:\n",
    "#   model.load_darknet_weights(f)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "isNo9Yg1NBe8"
   },
   "source": [
    "# model, config = get_model('yolov4-tiny')\n",
    "# download_data(\"https://github.com/zhiqwang/yolov5-rt-stack/releases/download/v0.3.0/coco128.zip\", \"data/\")\n",
    "# target_size = (config.height, config.width)\n",
    "# random_transforms = transforms.RandomApply(torch.nn.ModuleList([\n",
    "#                                 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#                                 transforms.RandomAffine(degrees = 30, translate=None, scale=None, shear=None, fill=0),\n",
    "#                                 transforms.Resize((config.height, config.width))                                                         \n",
    "# ]), p=0.5)\n",
    "# # transform = transforms.Compose([\n",
    "# #                                 # transforms.RandomResizedCrop((config.height, config.width)),\n",
    "# #                                 transforms.ToTensor(),\n",
    "# #                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "# #                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "# #                                 transforms.RandomPerspective(),\n",
    "# #                                 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "# #                                 # transforms.RandomAffine(degrees = 30, translate=None, scale=None, shear=None, fill=0),                                transforms.RandomResizedCrop(target_size, scale=(0.3, 1)),\n",
    "# #                                 transforms.RandomRotation(45, expand = True),\n",
    "# #                                 # transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0)),\n",
    "# #                                 # transforms.RandomErasing(),\n",
    "# #                                 transforms.Resize(target_size),\n",
    "# #                                 ])\n",
    "# transform = transforms.Compose([\n",
    "#                                 # transforms.RandomResizedCrop((config.height, config.width)),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 # transforms.RandomHorizontalFlip(p=0.5),\n",
    "#                                 # transforms.RandomVerticalFlip(p=0.5),\n",
    "#                                 # transforms.RandomPerspective(),\n",
    "#                                 # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#                                 # transforms.RandomAffine(degrees = 30, translate=None, scale=None, shear=None, fill=0),                                transforms.RandomResizedCrop(target_size, scale=(0.3, 1)),\n",
    "#                                 # transforms.RandomRotation(45, expand = True),\n",
    "#                                 # transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0)),\n",
    "#                                 # transforms.RandomErasing(),\n",
    "#                                 transforms.Resize(target_size),\n",
    "#                                 ])\n",
    "# datamodule = ObjectDetectionData.from_coco(\n",
    "#     train_folder=\"data/coco128/images/train2017/\",\n",
    "#     train_ann_file=\"data/coco128/annotations/instances_train2017.json\",\n",
    "#     batch_size=64,\n",
    "#     train_transform = transform\n",
    "#     # train_transform = transforms.Compose(transforms.Resize(config.height))\n",
    "# )\n",
    "# train_loader = datamodule.train_dataloader()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wcwQOU_IAhts"
   },
   "source": [
    "# class MyYOLO(YOLO):\n",
    "#     def __init__(self, name = 'yolov3-tiny'):\n",
    "#         config_path = os.path.join(CONFIG_PATH , name + '.cfg')\n",
    "#         weights_path = os.path.join(CONFIG_PATH , name + '.weights')\n",
    "#         if not os.path.exists(config_path):\n",
    "#             prepare_files(name)\n",
    "#         config = YOLOConfiguration(config_path)\n",
    "#         super().__init__(config.get_network())\n",
    "#         with open(weights_path, 'rb') as f:\n",
    "#             self.load_darknet_weights(f)\n",
    "        \n",
    "#     def predict(self, x):\n",
    "#         self.eval()\n",
    "#         detections = self(x)\n",
    "#         detections = self._split_detections(detections)\n",
    "#         detections = self._filter_detections(detections)\n",
    "#         return [{'boxes': boxes, 'scores': scores, 'labels': labels for boxes, scores, labels in zip(detections['boxes'],detections['scores'],detections['labels'])}]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8XnOEDHTRvkc"
   },
   "source": [
    "# MODEL_NAME = 'yolov4-tiny'\n",
    "# config_path = os.path.join(CONFIG_PATH , MODEL_NAME + '.cfg')\n",
    "# config = YOLOConfiguration(config_path)\n",
    "# model, config = get_model(MODEL_NAME)\n",
    "# target_size = (config.height, config.width)\n",
    "# transform = transforms.Compose([\n",
    "#                                 # transforms.RandomResizedCrop((config.height, config.width)),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 # transforms.RandomHorizontalFlip(p=0.5),\n",
    "#                                 # transforms.RandomVerticalFlip(p=0.5),\n",
    "#                                 # transforms.RandomPerspective(),\n",
    "#                                 # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#                                 # transforms.RandomAffine(degrees = 30, translate=None, scale=None, shear=None, fill=0),                                transforms.RandomResizedCrop(target_size, scale=(0.3, 1)),\n",
    "#                                 # transforms.RandomRotation(45, expand = True),\n",
    "#                                 # transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0)),\n",
    "#                                 # transforms.RandomErasing(),\n",
    "#                                 transforms.Resize(target_size),\n",
    "#                                 ])\n",
    "# pfdataset = PennFudanDataset('data/PennFudanPed', transforms = transform, download=True)\n",
    "# pfdatamodule = PennFudanModule()\n",
    "# pfdatamodule.setup()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RR0Pa9Y9nedx"
   },
   "source": [
    "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fRoOFS9w0DY_"
   },
   "source": [
    "# !pip install pytorch-lightning lightning-flash git+https://github.com/groke-technologies/pytorch-lightning-bolts.git@yolo --upgrade"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ra7kYCuLzq0i"
   },
   "source": [
    "# !pip install bypy==1.6.10"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5zBy5fE0HzM",
    "outputId": "eb7443de-51d6-4ed2-e48a-4edb87de665f"
   },
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import urllib\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "import zipfile\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "from PIL import Image\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.datasets as dset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "#pytroch-lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "    from flash import download_data\n",
    "    from flash.vision import ObjectDetectionData\n",
    "    from pl_bolts.models.detection import *\n",
    "    from pytorch_lightning.metrics.functional import accuracy\n",
    "    from pl_bolts.datasets import DummyDataset\n",
    "    from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "    from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "except:\n",
    "    !pip install pytorch-lightning lightning-flash git+https://github.com/groke-technologies/pytorch-lightning-bolts.git@yolo --upgrade\n",
    "    # !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
    "    import pytorch_lightning as pl\n",
    "    from flash import download_data\n",
    "    from flash.vision import ObjectDetectionData\n",
    "    from pl_bolts.models.detection import *\n",
    "    from pytorch_lightning.metrics.functional import accuracy\n",
    "    from pl_bolts.datasets import DummyDataset\n",
    "    from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "    from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n",
    "LOG_PATH = '/content/drive/MyDrive/logs'\n",
    "CONFIG_PATH = 'config'\n",
    "WEIGHTS_PATH = 'weights'\n",
    "MODELS= ['yolov3.cfg', 'yolov4.cfg', 'yolov4-tiny.cfg']\n",
    "CONFIG3_BASE_URL = 'https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/'\n",
    "WEIGHTS3_BASE_URL = 'https://pjreddie.com/media/files/'\n",
    "CONFIG4_BASE_URL = 'https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/'\n",
    "WEIGHTS4_BASE_URL = 'https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/'\n",
    "NAMES_URL = 'https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names'\n",
    "NAMES_PATH = os.path.join(CONFIG_PATH, 'coco.names')\n",
    "# WIDER_PERSON_ZIP = '/content/drive/MyDrive/datasets/WiderPerson.zip'\n",
    "# WIDER_PERSON_PATH = 'data/WiderPerson'\n",
    "WIDER_PERSON_ZIP = '/content/drive/MyDrive/datasets/LIP/TrainVal_images.zip'\n",
    "WIDER_PERSON_PATH = 'data/LIP'\n",
    "DHD_PATH = 'data/DHD'\n",
    "\n",
    "# if not os.path.exists(WIDER_PERSON_PATH):\n",
    "#     with zipfile.ZipFile(WIDER_PERSON_ZIP,\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(WIDER_PERSON_PATH)\n",
    "#     with zipfile.ZipFile(WIDER_PERSON_PATH + '/TrainVal_images.zip',\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(WIDER_PERSON_PATH + '/Images')\n",
    "COLORS = cm.get_cmap('hsv')(np.linspace(0, 1, 100))[np.newaxis, :, :3]\n",
    "COLORS = (COLORS * 255).astype('uint8')[0].tolist()\n",
    "COLORS = [tuple(color) for color in COLORS]\n",
    "def get_colors(num):\n",
    "  colors = cm.get_cmap('hsv')(np.linspace(0, 1, num))[np.newaxis, :, :3]\n",
    "  colors = (colors * 255).astype('uint8')[0].tolist()\n",
    "  colors = [tuple(color) for color in colors]\n",
    "  return colors\n",
    "def prepare_files(name):\n",
    "  if name.startswith('yolov3'):\n",
    "    config_base_url = CONFIG3_BASE_URL\n",
    "    weights_base_url = WEIGHTS3_BASE_URL\n",
    "  else:\n",
    "    config_base_url = CONFIG4_BASE_URL\n",
    "    weights_base_url = WEIGHTS4_BASE_URL\n",
    "  # cfg_path = os.path.join(CONFIG_PATH, config)\n",
    "  # weights_path = cfg_path.replace('cfg', 'weights')\n",
    "  cfg_url = config_base_url + name + '.cfg'\n",
    "  weights_url = weights_base_url + name + '.weights'\n",
    "  download_data(cfg_url, CONFIG_PATH)\n",
    "  download_data(weights_url, CONFIG_PATH)\n",
    "\n",
    "# for model in MODELS:\n",
    "#   prepare_files(model)\n",
    "def get_model(name):\n",
    "  config_path = os.path.join(CONFIG_PATH , name + '.cfg')\n",
    "  weights_path = os.path.join(CONFIG_PATH , name + '.weights')\n",
    "  if not os.path.exists(config_path):\n",
    "    prepare_files(name)\n",
    "  config = YOLOConfiguration(config_path)\n",
    "  model = YOLO(config.get_network())\n",
    "  with open(weights_path, 'rb') as f:\n",
    "    model.load_darknet_weights(f)\n",
    "  return model, config"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ba41d630939b>:16: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-ba41d630939b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrcParams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'lines.linewidth'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q89YPdfq0a_E"
   },
   "source": [
    "# if not os.path.exists(DHD_PATH):\n",
    "#     from bypy import ByPy\n",
    "#     bp=ByPy()\n",
    "#     bp.list()\n",
    "#     bp.downdir('DHD/', 'data/DHD')\n",
    "#     with zipfile.ZipFile(DHD_PATH + '/TrainVal_images.zip',\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(DHD_PATH + '/Images')\n",
    "# with zipfile.ZipFile(DHD_PATH + '/back_dhd_coco.zip',\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(DHD_PATH + '/Images')"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SsdVBrC2OFKN"
   },
   "source": [
    "def get_names():\n",
    "  download_data(NAMES_URL, CONFIG_PATH)\n",
    "  with open(NAMES_PATH) as f:\n",
    "    names = f.read().split('\\n')\n",
    "  names = list(filter(None, names))\n",
    "  names2id = {name:id for id, name in enumerate(names)}\n",
    "  return names, names2id\n",
    "NAMES, NAME2ID = get_names()"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LxAxk9N-fspV"
   },
   "source": [
    "def infer(model, image, device = 'cpu'):\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = F.to_tensor(image)\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    model.eval()\n",
    "    detections = model(image)\n",
    "    detections = model._split_detections(detections)\n",
    "    detections = model._filter_detections(detections)\n",
    "    return list(zip(detections['boxes'],detections['scores'],detections['labels'], image))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GbyKDALtPTzO"
   },
   "source": [
    "def process_result(result, target= ['person'], device = 'cpu', show = True):\n",
    "    if device:\n",
    "        result = (i.to(device) for i in result)\n",
    "        # image = image.to(device)\n",
    "    bbox, confidence, id, image = result\n",
    "    if target:\n",
    "        mask = torch.tensor([NAME2ID[i] for i in target],dtype = torch.long, device = device).unsqueeze(0).T\n",
    "        mask = torch.any(id.eq(mask), 0)\n",
    "        bbox = bbox[mask]\n",
    "        confidence = confidence[mask]\n",
    "        id = id[mask]\n",
    "    for i in range(len(id)):\n",
    "        print(\"Found a {} at ({}, {}) of confidence {:.3f}\".format(\n",
    "            NAMES[int(id[i].item())], \n",
    "            int((bbox[i][0].item() + bbox[i][2].item()) / 2),\n",
    "            int((bbox[i][1].item() + bbox[i][3].item()) / 2),\n",
    "            confidence[i].item()\n",
    "        ))\n",
    "    if show:\n",
    "        image = (image * 255).detach().cpu().byte()\n",
    "        labels = [NAMES[i.item()] for i in id]\n",
    "        bbox_image = draw_bounding_boxes(image, bbox, labels, width = 3, colors = get_colors(len(id)), font = \"LiberationMono-Bold.ttf\", font_size = 30)\n",
    "        plt.figure()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(torch.cat((image, bbox_image), 2).permute(1, 2, 0).detach().numpy())\n",
    "        plt.show()\n",
    "    return bbox, confidence, id\n",
    "\n",
    "def process_results(results, target = ['person'], device = 'cpu'):\n",
    "    confidences = []\n",
    "    # ids = []\n",
    "    for i, result in enumerate(results):\n",
    "        bbox, confidence, id, image = result\n",
    "        if target:\n",
    "            mask = torch.tensor([NAME2ID[i] for i in target],dtype = torch.long, device = device).unsqueeze(0).T\n",
    "            mask = torch.any(id.eq(mask), 0)\n",
    "            # confidence = confidence[idx]\n",
    "            confidence = confidence[mask]\n",
    "            # id = id[mask]\n",
    "            confidences.append(confidence)\n",
    "    return torch.cat(confidences)\n"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D1RgLs0EhA6O"
   },
   "source": [
    "# train_loader = DataLoader(pfdataset, batch_size=10)\n",
    "# imgs = next(iter(train_loader))\n",
    "# imgs.requires_grad = True\n",
    "# model.requires_grad = False\n",
    "# results = infer(model, imgs, device = device)\n",
    "# confidences = process_results(results, device = device)\n",
    "# for result in results:\n",
    "#   process_result(result, device = device)\n",
    "# for *result, img in zip(*result, imgs):\n",
    "#   process_result(result, img)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7VOkFQjT8nJ"
   },
   "source": [
    "def apply_patch(patch, image, patch_size = 100, position = None):\n",
    "    assert(patch_size < image.shape[-2] and patch_size < image.shape[-1])\n",
    "    if patch_size:\n",
    "        patch = transforms.Resize(patch_size)(patch)\n",
    "    else:\n",
    "        patch = transforms.Resize((position[2] - position[0], position[3] - position[1]))(patch)\n",
    "    if not position:\n",
    "        x1 = np.random.randint(0, image.shape[-2] - patch_size - 1)\n",
    "        y1 = np.random.randint(0, image.shape[-1] - patch_size - 1)\n",
    "        position = [x1, y1, x1 + patch_size, y1 + patch_size]\n",
    "    image = image.detach().clone()\n",
    "    image[...,:,position[0]:position[2], position[1] : position[3]] = patch\n",
    "    return image"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C7A6ZAB2Ith2"
   },
   "source": [
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root = 'data/PennFudanPed', transforms = None, download = True):\n",
    "        if download:\n",
    "            download_data('https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip', 'data')\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class WiderPersonDataset(object):\n",
    "    def __init__(self, root = 'data/WiderPerson', transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "        \n",
    "class PersonDataset(object):\n",
    "    def __init__(self, root = 'data/LIP', transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images/train_images\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"Images/train_images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class DHDDataset(object):\n",
    "    def __init__(self, root = 'data/DHD', transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        with open(os.path.join(DHD_PATH, 'dhd_coco.json')) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"Images\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img#, self.data[self.imgs[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class ImageDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, dataset, target_size = (608, 608), batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        # self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms.Compose([            \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Resize(target_size),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        dset = dataset(transforms = self.transforms)\n",
    "        train_size = int(float(len(dset)) * 0.9)\n",
    "        val_size = len(dset) - train_size\n",
    "        self.train, self.val = random_split(dset, [train_size,val_size])\n",
    "\n",
    "    # def setup(self, stage = None):\n",
    "        # self.pf_test = PennFudanDataset(self.data_dir, train=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.pf_test, batch_size=self.batch_size)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xp2v2nmQUSdi"
   },
   "source": [
    "class PatchNet(pl.LightningModule):\n",
    "    def __init__(self, patch_size = 100):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.patch = torch.nn.Parameter(torch.rand((3, self.patch_size, self.patch_size)), requires_grad=True)\n",
    "        self.register_parameter(name='patch', param=self.patch)\n",
    "\n",
    "\n",
    "    def apply_patch(self, image, position = None, patch_size = 100, position_scale = 608 / 416):\n",
    "        # assert(patch_size < image.shape[-2] and patch_size < image.shape[-1])\n",
    "        # if patch_size:\n",
    "        #     patch = transforms.Resize(patch_size)(patch)\n",
    "        # else:\n",
    "        #     patch = transforms.Resize((position[2] - position[0], position[3] - position[1]))(patch)\n",
    "        if not position:\n",
    "            x1 = np.random.randint(0, image.shape[-2] - patch_size - 1)\n",
    "            y1 = np.random.randint(0, image.shape[-1] - patch_size - 1)\n",
    "            position = [x1, y1, x1 + patch_size, y1 + patch_size]\n",
    "        else:\n",
    "            x1 = np.random.randint(position[0] * position_scale, max(position[2] * position_scale, image.shape[-2] - patch_size - 1))\n",
    "            y1 = np.random.randint(position[1] * position_scale, max(position[3] * position_scale, image.shape[-1] - patch_size - 1))\n",
    "            position = [x1, y1, x1 + patch_size, y1 + patch_size]\n",
    "        image = image.detach().clone()\n",
    "        image[...,:,position[0]:position[2], position[1] : position[3]] = self.patch.clip(0,1)\n",
    "        return image\n",
    "\n",
    "    def forward(self, x, position = None):\n",
    "        x = self.apply_patch(x, position)\n",
    "        return x"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qoy9A5LAD0zK"
   },
   "source": [
    "class PatchAdvNet(pl.LightningModule):\n",
    "    def __init__(self, model_name = 'yolov4-tiny', dataset = WiderPersonDataset, patch_size = 100, batch_size = 64, datamodule = ImageDataModule):\n",
    "        super().__init__()\n",
    "        self.yolo, self.config = get_model(model_name)\n",
    "        target_size = (self.config.width, self.config.height)\n",
    "        self.patch_size = patch_size\n",
    "        self.automatic_optimization = True\n",
    "        self.reset_patch()\n",
    "        self.batch_size = batch_size\n",
    "        # self.datamodule = datamodule(dataset)\n",
    "        self.lastpatch = None\n",
    "        # self.bce = torch.nn.BCELoss()\n",
    "        # self.train_dataloader, self.val_dataloader = datamodule.train_dataloader, datamodule.val_dataloader\n",
    "        self.transforms = transforms.Compose([            \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Resize(target_size),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "        dset = dataset(transforms = self.transforms)\n",
    "        train_size = int(float(len(dset)) * 0.9)\n",
    "        val_size = len(dset) - train_size\n",
    "        self.train_set, self.val_set = random_split(dset, [train_size,val_size])\n",
    "        \n",
    "    def reset_patch(self):\n",
    "        self.patch = PatchNet(self.patch_size)\n",
    "\n",
    "    def _infer(self, image):\n",
    "        self.yolo.eval()\n",
    "        detections = self.yolo(image)\n",
    "        detections = self.yolo._split_detections(detections)\n",
    "        detections = self.yolo._filter_detections(detections)\n",
    "        return list(zip(detections['boxes'],detections['scores'],detections['labels']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch(x)\n",
    "        x = self._infer(x)\n",
    "        return x\n",
    "\n",
    "    def get_loss(self, results, target = 'person'):\n",
    "        confidences = []\n",
    "        for i, result in enumerate(results):\n",
    "            bbox, confidence, id = result\n",
    "            if target:\n",
    "                # mask = torch.tensor([NAME2ID[i] for i in target],dtype = torch.long).unsqueeze(0).T\n",
    "                # mask = torch.any(id.eq(mask), 0)\n",
    "                mask = id.eq(NAME2ID[target])\n",
    "                confidence = confidence[mask]\n",
    "                confidences.append(confidence)\n",
    "            # confidences.append(confidence)\n",
    "        confidences = torch.cat(confidences).mean() if confidences else torch.tensor(0, dtype = torch.float32)\n",
    "        # target = torch.zeros_like(confidences)\n",
    "        # return self.bce(confidences, target)\n",
    "        return confidences.sum()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # if type(batch) is tuple:\n",
    "        #     x, position = batch\n",
    "        #     x = self.patch(x, position)\n",
    "        # else:\n",
    "        x = batch\n",
    "        x = self.patch(x)\n",
    "        x = self._infer(x)\n",
    "        loss = self.get_loss(x)\n",
    "        self.log('train_loss', loss)\n",
    "        if (batch_idx %1000 == 999):\n",
    "            patch = self.patch.patch.clone().detach().cpu().permute((1, 2, 0))\n",
    "            if self.lastpatch != None:\n",
    "                delta = patch - self.lastpatch\n",
    "                print(delta.max())\n",
    "            self.lastpatch = patch\n",
    "            plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(patch)\n",
    "            plt.show()\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # if type(batch) is tuple:\n",
    "        #     x, position = batch\n",
    "        #     x = self.patch(x, position)\n",
    "        # else:\n",
    "        x = batch\n",
    "        x = self.patch(x)\n",
    "        x = self._infer(x)\n",
    "        loss = self.get_loss(x)\n",
    "        self.log('val_loss', loss)\n",
    "        if (batch_idx %1000 == 999):\n",
    "            img = x.clone().detach().cpu().permute((1, 2, 0))\n",
    "            plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(x)\n",
    "            plt.show()\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.patch.parameters(), lr=1e-3)\n",
    "        # scheduler = optim.lr_scheduler.\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'train_confidence',\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "id": "FrBDSiBYhVmV",
    "outputId": "435353ec-ece8-4cdd-889e-2081ce425373"
   },
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir drive/MyDrive/logs/default/"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 240), started 0:37:25 ago. (Use '!kill 240' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "93196f85e7ff40a98e94696ffb273e6c",
      "06fa7b72b6e645609e0e841afc68860c",
      "adbaeca5a59b4b90975589a70764ad58",
      "08b409691db144ebaeab1a7c567fcab3",
      "732c0f4d99ac42ee909946318101e1f8",
      "89ad58ec717a48fcb94dd1f38e7faeb9",
      "a80f1cefee004d7fa9d67622b8af49ef",
      "e8d681f3a6cc426a9906fc37cb425fd3",
      "0fad82b7365249f4aeb85afedb60e44c",
      "54f2c4c6e833427083d96bdb1ef8ba03",
      "bad5359b1b194503ad9aa39a38d36d69",
      "b9e7e380d86342f789531674320b2137",
      "eb939163d6c3440e9ff6ba3146ac2ef0",
      "2eb37042f4e84086a153e0941f68c19b",
      "e1d8bb242d12453c89089743643770fb",
      "880cc5ca1b72453bb97206e30c04ca28"
     ]
    },
    "id": "p6RT1OKrYLzg",
    "outputId": "e81960ea-a3bd-4488-acd5-0f919a2aefd8"
   },
   "source": [
    "try:\n",
    "    os.mkdir('data')\n",
    "    os.symlink('/content/drive/MyDrive/datasets/DHD', 'data/DHD')\n",
    "except:\n",
    "    pass\n",
    "model = PatchAdvNet(dataset = DHDDataset)\n",
    "# persondataset = PersonDataset\n",
    "# train_loader = DataLoader(person, batch_size = 1)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(LOG_PATH)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='/content/drive/MyDrive/checkpoints',\n",
    "    monitor='train_loss',\n",
    "    filename='basic_confidence_patch-{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus = 1, \n",
    "    auto_scale_batch_size = 'power', \n",
    "    logger = tb_logger,\n",
    "    # resume_from_checkpoint = '/content/drive/MyDrive/checkpoints',\n",
    "    callbacks=[checkpoint_callback]\n",
    "    )\n",
    "trainer.tune(model)\n",
    "print(model.batch_size)\n",
    "trainer.fit(model)\n",
    "# trainer.fit(patch_net, train_loader)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "Batch size 9586 succeeded, trying batch size 19172\n",
      "Finished batch size finder, will continue with full run using batch size 9586\n",
      "Restored states from the checkpoint file at /content/scale_batch_size_temp_model.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | yolo  | YOLO     | 6.1 M \n",
      "1 | patch | PatchNet | 30.0 K\n",
      "-----------------------------------\n",
      "6.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.1 M     Total params\n",
      "24.346    Total estimated model params size (MB)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "9586\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93196f85e7ff40a98e94696ffb273e6c",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layoutâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\r"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fad82b7365249f4aeb85afedb60e44c",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WSGDtJ-k1Iy0"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-XEQKE9_8i9H"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}